{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8fcde9-5ae9-46aa-a5fd-bc86e79155b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model and class map…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes: ['Blur', 'Copy Paste', 'CopyPaste+Blur', 'CopyPaste+Insertion', 'CopyPaste+Noise', 'Insertion', 'Insertion+Blur', 'Insertion+Noise', 'Noise', 'Normal']\n",
      "[INFO] Scanning Demo folder: C:\\Users\\sagni\\Downloads\\Docu Verify\\Forged Handwritten Document Database\\Forged Handwritten Document Database\\Handwritten Forged Document Dataset 2023\\Demo\n",
      "[INFO] Found 33 image(s)\n",
      "[1/33] InsNoise(10).jpg → pred=Insertion+Blur (18.3%) | GT=UNKNOWN\n",
      "[2/33] InsNoise(11).jpg → pred=Insertion+Blur (18.3%) | GT=UNKNOWN\n",
      "[3/33] InsNoise(12).jpg → pred=Insertion+Blur (18.6%) | GT=UNKNOWN\n",
      "[4/33] InsNoise(13).jpg → pred=Insertion+Blur (17.4%) | GT=UNKNOWN\n",
      "[5/33] InsNoise(16).jpg → pred=Insertion+Blur (18.0%) | GT=UNKNOWN\n",
      "[6/33] InsNoise(17).jpg → pred=Insertion+Blur (15.1%) | GT=UNKNOWN\n",
      "[7/33] InsNoise(19).jpg → pred=Insertion+Blur (16.1%) | GT=UNKNOWN\n",
      "[8/33] InsNoise(4).jpg → pred=Insertion+Blur (14.9%) | GT=UNKNOWN\n",
      "[9/33] InsNoise(5).jpg → pred=Insertion+Blur (14.6%) | GT=UNKNOWN\n",
      "[10/33] InsNoise(8).jpg → pred=Insertion+Blur (17.3%) | GT=UNKNOWN\n",
      "[11/33] Noise(10).jpg → pred=Insertion+Blur (18.4%) | GT=UNKNOWN\n",
      "[12/33] Noise(11).jpg → pred=Insertion+Blur (18.4%) | GT=UNKNOWN\n",
      "[13/33] Noise(13).jpg → pred=Insertion+Blur (17.4%) | GT=UNKNOWN\n",
      "[14/33] Noise(14).jpg → pred=Insertion+Blur (18.3%) | GT=UNKNOWN\n",
      "[15/33] Noise(16).jpg → pred=Insertion+Blur (17.9%) | GT=UNKNOWN\n",
      "[16/33] Noise(17).jpg → pred=Insertion+Blur (15.1%) | GT=UNKNOWN\n",
      "[17/33] Noise(4).jpg → pred=Insertion+Blur (14.9%) | GT=UNKNOWN\n",
      "[18/33] Noise(5).jpg → pred=Insertion+Blur (14.6%) | GT=UNKNOWN\n",
      "[19/33] Noise(7).jpg → pred=Insertion+Blur (20.5%) | GT=UNKNOWN\n",
      "[20/33] Noise(8).jpg → pred=Insertion+Blur (17.2%) | GT=UNKNOWN\n",
      "[21/33] Normal(30).jpg → pred=Insertion+Blur (17.5%) | GT=UNKNOWN\n",
      "[22/33] Normal(31).jpg → pred=Insertion+Blur (17.3%) | GT=UNKNOWN\n",
      "[23/33] Normal(32).jpg → pred=Insertion+Blur (18.1%) | GT=UNKNOWN\n",
      "[24/33] Normal(36).jpg → pred=Insertion+Blur (18.9%) | GT=UNKNOWN\n",
      "[25/33] Normal(37).jpg → pred=Insertion+Blur (16.3%) | GT=UNKNOWN\n",
      "[26/33] Normal(38).jpg → pred=Insertion+Blur (19.5%) | GT=UNKNOWN\n",
      "[27/33] Normal(39).jpg → pred=Insertion+Blur (16.7%) | GT=UNKNOWN\n",
      "[28/33] Normal(40).jpg → pred=Insertion+Blur (18.5%) | GT=UNKNOWN\n",
      "[29/33] Normal(41).jpg → pred=Insertion+Blur (17.8%) | GT=UNKNOWN\n",
      "[30/33] Normal(45).jpg → pred=Insertion+Blur (17.0%) | GT=UNKNOWN\n",
      "[31/33] Normal(46).jpg → pred=Insertion+Blur (19.4%) | GT=UNKNOWN\n",
      "[32/33] Normal(47).jpg → pred=Noise (14.0%) | GT=UNKNOWN\n",
      "[33/33] Normal(48).jpg → pred=Insertion+Blur (14.3%) | GT=UNKNOWN\n",
      "[INFO] Saved predictions CSV → C:\\Users\\sagni\\Downloads\\Docu Verify\\demo_predictions.csv\n",
      "[INFO] Saved predictions JSON → C:\\Users\\sagni\\Downloads\\Docu Verify\\demo_predictions.json\n",
      "[INFO] Saved summary → C:\\Users\\sagni\\Downloads\\Docu Verify\\demo_summary.json\n",
      "\n",
      "=== DONE ===\n",
      "Annotated images → C:\\Users\\sagni\\Downloads\\Docu Verify\\annotated_demo\n",
      "Misclassified copies → C:\\Users\\sagni\\Downloads\\Docu Verify\\mistakes_demo\n"
     ]
    }
   ],
   "source": [
    "import os, json, csv, shutil, pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG (Windows paths)\n",
    "# ----------------------------\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\sagni\\Downloads\\Docu Verify\")\n",
    "MODEL_H5   = OUTPUT_DIR / \"model.h5\"\n",
    "CLASS_PKL  = OUTPUT_DIR / \"class_indices.pkl\"\n",
    "\n",
    "DEMO_DIR = Path(\n",
    "    r\"C:\\Users\\sagni\\Downloads\\Docu Verify\\Forged Handwritten Document Database\\Forged Handwritten Document Database\\Handwritten Forged Document Dataset 2023\\Demo\"\n",
    ")\n",
    "\n",
    "IMG_SIZE   = (256, 256)\n",
    "TOP_K      = 3\n",
    "ANNOTATE   = True          # draw predicted label + confidence on images\n",
    "COPY_MISTAKES = True       # copy misclassified images for quick review\n",
    "ANN_DIR    = OUTPUT_DIR / \"annotated_demo\"\n",
    "MISTAKE_DIR= OUTPUT_DIR / \"mistakes_demo\"\n",
    "\n",
    "CSV_OUT    = OUTPUT_DIR / \"demo_predictions.csv\"\n",
    "JSON_OUT   = OUTPUT_DIR / \"demo_predictions.json\"\n",
    "SUMMARY_OUT= OUTPUT_DIR / \"demo_summary.json\"\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def ensure_paths():\n",
    "    if not MODEL_H5.exists():\n",
    "        raise FileNotFoundError(f\"Missing model: {MODEL_H5}\")\n",
    "    if not CLASS_PKL.exists():\n",
    "        raise FileNotFoundError(f\"Missing class map: {CLASS_PKL}\")\n",
    "    if not DEMO_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Demo folder not found: {DEMO_DIR}\")\n",
    "\n",
    "def load_class_indices(pkl_path: Path) -> Tuple[Dict[str,int], Dict[int,str], List[str]]:\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        class_indices: Dict[str,int] = pickle.load(f)\n",
    "    idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "    ordered = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "    return class_indices, idx_to_class, ordered\n",
    "\n",
    "def list_images(path: Path) -> List[Path]:\n",
    "    exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
    "    if path.is_file():\n",
    "        return [path] if path.suffix.lower() in exts else []\n",
    "    return sorted([p for p in path.rglob(\"*\") if p.suffix.lower() in exts])\n",
    "\n",
    "def load_tensor(img_path: Path) -> np.ndarray:\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    arr = preprocess_input(arr)\n",
    "    return np.expand_dims(arr, axis=0)  # (1,H,W,3)\n",
    "\n",
    "def annotate(img_path: Path, text: str, out_path: Path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    margin = 8\n",
    "    try:\n",
    "        bbox = draw.textbbox((0,0), text, font=font)\n",
    "        tw, th = bbox[2]-bbox[0], bbox[3]-bbox[1]\n",
    "    except:\n",
    "        tw, th = int(draw.textlength(text, font=font)), 24\n",
    "    bw, bh = tw + 2*margin, th + 2*margin\n",
    "    draw.rectangle([(10,10),(10+bw,10+bh)], fill=(0,0,0,180))\n",
    "    draw.text((10+margin,10+margin), text, font=font, fill=(255,255,255))\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    img.save(out_path)\n",
    "\n",
    "def infer_truth_from_parent(img_path: Path, known_classes: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    If Demo images are stored like Demo\\<TrueLabel>\\image.jpg,\n",
    "    we use the immediate parent folder name as the ground truth.\n",
    "    If parent name doesn't match known classes, return \"\".\n",
    "    \"\"\"\n",
    "    parent = img_path.parent.name\n",
    "    # match exact first, then case-insensitive fallback\n",
    "    if parent in known_classes:\n",
    "        return parent\n",
    "    for c in known_classes:\n",
    "        if c.lower() == parent.lower():\n",
    "            return c\n",
    "    return \"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    ensure_paths()\n",
    "\n",
    "    print(\"[INFO] Loading model and class map…\")\n",
    "    model = load_model(str(MODEL_H5))\n",
    "    class_indices, idx_to_class, ordered_classes = load_class_indices(CLASS_PKL)\n",
    "    num_classes = len(ordered_classes)\n",
    "    k = min(TOP_K, num_classes)\n",
    "    print(\"[INFO] Classes:\", ordered_classes)\n",
    "\n",
    "    print(f\"[INFO] Scanning Demo folder: {DEMO_DIR}\")\n",
    "    files = list_images(DEMO_DIR)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No images found under: {DEMO_DIR}\")\n",
    "    print(f\"[INFO] Found {len(files)} image(s)\")\n",
    "\n",
    "    rows = []\n",
    "    per_class_totals   = Counter()\n",
    "    per_class_corrects = Counter()\n",
    "    overall_correct = 0\n",
    "    overall_total   = 0\n",
    "\n",
    "    for i, img_path in enumerate(files, start=1):\n",
    "        x = load_tensor(img_path)\n",
    "        probs = model.predict(x, verbose=0)[0]  # (C,)\n",
    "\n",
    "        top_idx = np.argsort(probs)[::-1][:k]\n",
    "        top_classes = [idx_to_class[int(t)] for t in top_idx]\n",
    "        top_scores  = [float(probs[int(t)]) for t in top_idx]\n",
    "        pred_class, pred_conf = top_classes[0], top_scores[0]\n",
    "\n",
    "        # infer ground truth from parent folder\n",
    "        true_label = infer_truth_from_parent(img_path, ordered_classes)\n",
    "\n",
    "        correct = None\n",
    "        if true_label != \"\":\n",
    "            correct = (pred_class == true_label)\n",
    "            overall_total += 1\n",
    "            if correct:\n",
    "                overall_correct += 1\n",
    "                per_class_corrects[true_label] += 1\n",
    "            per_class_totals[true_label] += 1\n",
    "\n",
    "        row = {\n",
    "            \"file\": str(img_path),\n",
    "            \"true_label\": true_label,\n",
    "            \"pred_class\": pred_class,\n",
    "            \"confidence\": round(pred_conf, 6),\n",
    "            \"correct\": (None if correct is None else bool(correct))\n",
    "        }\n",
    "        for j, (c, s) in enumerate(zip(top_classes, top_scores), start=1):\n",
    "            row[f\"top{j}_class\"] = c\n",
    "            row[f\"top{j}_p\"] = round(s, 6)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        # annotate\n",
    "        if ANNOTATE:\n",
    "            tag = f\"{pred_class} ({pred_conf*100:.1f}%)\"\n",
    "            if true_label:\n",
    "                tag += f\" | GT: {true_label} | {'✓' if correct else '✗'}\"\n",
    "            out_img = ANN_DIR / f\"{img_path.stem}_pred.png\"\n",
    "            try:\n",
    "                annotate(img_path, tag, out_img)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Annotate failed for {img_path.name}: {e}\")\n",
    "\n",
    "        # copy mistakes for quick review\n",
    "        if COPY_MISTAKES and correct is False and true_label:\n",
    "            dst_dir = MISTAKE_DIR / true_label\n",
    "            dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "            try:\n",
    "                shutil.copy2(img_path, dst_dir / img_path.name)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not copy mistake {img_path.name}: {e}\")\n",
    "\n",
    "        # console preview\n",
    "        if true_label:\n",
    "            print(f\"[{i}/{len(files)}] {img_path.name} → pred={pred_class} ({pred_conf*100:.1f}%) | GT={true_label} | {'CORRECT' if correct else 'WRONG'}\")\n",
    "        else:\n",
    "            print(f\"[{i}/{len(files)}] {img_path.name} → pred={pred_class} ({pred_conf*100:.1f}%) | GT=UNKNOWN\")\n",
    "\n",
    "    # --- Save CSV/JSON\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(CSV_OUT, index=False, encoding=\"utf-8\")\n",
    "    with open(JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rows, f, indent=2)\n",
    "    print(f\"[INFO] Saved predictions CSV → {CSV_OUT}\")\n",
    "    print(f\"[INFO] Saved predictions JSON → {JSON_OUT}\")\n",
    "\n",
    "    # --- Summary with accuracy (if GT available)\n",
    "    summary = {\n",
    "        \"demo_dir\": str(DEMO_DIR),\n",
    "        \"num_images\": len(rows),\n",
    "        \"top_k\": k,\n",
    "        \"annotated_dir\": str(ANN_DIR) if ANNOTATE else \"\",\n",
    "        \"mistakes_dir\": str(MISTAKE_DIR) if COPY_MISTAKES else \"\",\n",
    "        \"per_class_counts\": df[\"pred_class\"].value_counts().to_dict(),\n",
    "    }\n",
    "\n",
    "    if overall_total > 0:\n",
    "        overall_acc = overall_correct / overall_total\n",
    "        per_class_acc = {c: (per_class_corrects[c] / per_class_totals[c]) if per_class_totals[c] > 0 else None\n",
    "                         for c in ordered_classes}\n",
    "        summary.update({\n",
    "            \"gt_images\": overall_total,\n",
    "            \"overall_top1_accuracy\": round(overall_acc, 6),\n",
    "            \"per_class_accuracy\": {k: (None if v is None else round(v, 6)) for k, v in per_class_acc.items()}\n",
    "        })\n",
    "    else:\n",
    "        summary.update({\"gt_images\": 0, \"overall_top1_accuracy\": None, \"per_class_accuracy\": {c: None for c in ordered_classes}})\n",
    "\n",
    "    with open(SUMMARY_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"[INFO] Saved summary → {SUMMARY_OUT}\")\n",
    "\n",
    "    print(\"\\n=== DONE ===\")\n",
    "    if ANNOTATE:\n",
    "        print(f\"Annotated images → {ANN_DIR}\")\n",
    "    if COPY_MISTAKES:\n",
    "        print(f\"Misclassified copies → {MISTAKE_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb79014d-c10c-463c-aec5-931ffe2b24e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
