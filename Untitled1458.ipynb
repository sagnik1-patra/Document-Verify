{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3317c9a9-a3d3-4036-9ad6-5ba0397fce3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model and classes…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Classes: ['Blur', 'Copy Paste', 'CopyPaste+Blur', 'CopyPaste+Insertion', 'CopyPaste+Noise', 'Insertion', 'Insertion+Blur', 'Insertion+Noise', 'Noise', 'Normal']\n",
      "[INFO] Scanning Demo folder: C:\\Users\\sagni\\Downloads\\Docu Verify\\Forged Handwritten Document Database\\Forged Handwritten Document Database\\Handwritten Forged Document Dataset 2023\\Demo\n",
      "[INFO] Found 33 image(s)\n",
      "[1/33] InsNoise(10).jpg → Insertion+Blur (18.34%)\n",
      "[2/33] InsNoise(11).jpg → Insertion+Blur (18.26%)\n",
      "[3/33] InsNoise(12).jpg → Insertion+Blur (18.61%)\n",
      "[4/33] InsNoise(13).jpg → Insertion+Blur (17.41%)\n",
      "[5/33] InsNoise(16).jpg → Insertion+Blur (18.02%)\n",
      "[6/33] InsNoise(17).jpg → Insertion+Blur (15.08%)\n",
      "[7/33] InsNoise(19).jpg → Insertion+Blur (16.07%)\n",
      "[8/33] InsNoise(4).jpg → Insertion+Blur (14.93%)\n",
      "[9/33] InsNoise(5).jpg → Insertion+Blur (14.61%)\n",
      "[10/33] InsNoise(8).jpg → Insertion+Blur (17.29%)\n",
      "[11/33] Noise(10).jpg → Insertion+Blur (18.40%)\n",
      "[12/33] Noise(11).jpg → Insertion+Blur (18.39%)\n",
      "[13/33] Noise(13).jpg → Insertion+Blur (17.41%)\n",
      "[14/33] Noise(14).jpg → Insertion+Blur (18.32%)\n",
      "[15/33] Noise(16).jpg → Insertion+Blur (17.95%)\n",
      "[16/33] Noise(17).jpg → Insertion+Blur (15.15%)\n",
      "[17/33] Noise(4).jpg → Insertion+Blur (14.87%)\n",
      "[18/33] Noise(5).jpg → Insertion+Blur (14.60%)\n",
      "[19/33] Noise(7).jpg → Insertion+Blur (20.48%)\n",
      "[20/33] Noise(8).jpg → Insertion+Blur (17.23%)\n",
      "[21/33] Normal(30).jpg → Insertion+Blur (17.51%)\n",
      "[22/33] Normal(31).jpg → Insertion+Blur (17.31%)\n",
      "[23/33] Normal(32).jpg → Insertion+Blur (18.06%)\n",
      "[24/33] Normal(36).jpg → Insertion+Blur (18.89%)\n",
      "[25/33] Normal(37).jpg → Insertion+Blur (16.32%)\n",
      "[26/33] Normal(38).jpg → Insertion+Blur (19.53%)\n",
      "[27/33] Normal(39).jpg → Insertion+Blur (16.66%)\n",
      "[28/33] Normal(40).jpg → Insertion+Blur (18.52%)\n",
      "[29/33] Normal(41).jpg → Insertion+Blur (17.82%)\n",
      "[30/33] Normal(45).jpg → Insertion+Blur (16.98%)\n",
      "[31/33] Normal(46).jpg → Insertion+Blur (19.36%)\n",
      "[32/33] Normal(47).jpg → Noise (14.02%)\n",
      "[33/33] Normal(48).jpg → Insertion+Blur (14.31%)\n",
      "[INFO] Saved JSON → C:\\Users\\sagni\\Downloads\\Docu Verify\\predictions.json\n",
      "[INFO] Saved CSV → C:\\Users\\sagni\\Downloads\\Docu Verify\\predictions.csv\n",
      "[INFO] Saved class count bar chart → C:\\Users\\sagni\\Downloads\\Docu Verify\\class_counts.png\n",
      "[INFO] Saved summary → C:\\Users\\sagni\\Downloads\\Docu Verify\\summary.json\n",
      "[INFO] Annotated images saved in → C:\\Users\\sagni\\Downloads\\Docu Verify\\annotated_predictions\n",
      "\n",
      "[DONE] Prediction complete.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, csv, pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG (Windows paths)\n",
    "# ----------------------------\n",
    "OUTPUT_DIR = Path(r\"C:\\Users\\sagni\\Downloads\\Docu Verify\")\n",
    "MODEL_H5   = OUTPUT_DIR / \"model.h5\"\n",
    "CLASS_PKL  = OUTPUT_DIR / \"class_indices.pkl\"\n",
    "\n",
    "# Your Demo folder (recursively scanned)\n",
    "DEMO_DIR = Path(\n",
    "    r\"C:\\Users\\sagni\\Downloads\\Docu Verify\\Forged Handwritten Document Database\\Forged Handwritten Document Database\\Handwritten Forged Document Dataset 2023\\Demo\"\n",
    ")\n",
    "\n",
    "# Inference options\n",
    "IMG_SIZE   = (256, 256)\n",
    "TOP_K      = 3\n",
    "ANNOTATE   = True\n",
    "ANN_DIR    = OUTPUT_DIR / \"annotated_predictions\"\n",
    "\n",
    "# Outputs\n",
    "JSON_OUT   = OUTPUT_DIR / \"predictions.json\"\n",
    "CSV_OUT    = OUTPUT_DIR / \"predictions.csv\"\n",
    "SUMMARY_JSON = OUTPUT_DIR / \"summary.json\"\n",
    "BAR_PNG    = OUTPUT_DIR / \"class_counts.png\"\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def ensure_artifacts():\n",
    "    if not MODEL_H5.exists():\n",
    "        raise FileNotFoundError(f\"Missing model: {MODEL_H5}\")\n",
    "    if not CLASS_PKL.exists():\n",
    "        raise FileNotFoundError(f\"Missing class map: {CLASS_PKL}\")\n",
    "    if not DEMO_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Demo folder not found: {DEMO_DIR}\")\n",
    "\n",
    "def load_class_indices(pkl_path: Path) -> Tuple[Dict[str,int], Dict[int,str], List[str]]:\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        class_indices: Dict[str,int] = pickle.load(f)  # {'ClassName': index, ...}\n",
    "    idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "    ordered_classes = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "    return class_indices, idx_to_class, ordered_classes\n",
    "\n",
    "def list_images(path: Path) -> List[Path]:\n",
    "    exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".webp\"}\n",
    "    if path.is_file():\n",
    "        return [path] if path.suffix.lower() in exts else []\n",
    "    return sorted([p for p in path.rglob(\"*\") if p.suffix.lower() in exts])\n",
    "\n",
    "def load_tensor(img_path: Path) -> np.ndarray:\n",
    "    img = Image.open(img_path).convert(\"RGB\").resize(IMG_SIZE)\n",
    "    arr = np.array(img).astype(np.float32)\n",
    "    arr = preprocess_input(arr)\n",
    "    return np.expand_dims(arr, axis=0)  # (1,H,W,3)\n",
    "\n",
    "def annotate_image(img_path: Path, label_text: str, out_path: Path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    margin = 8\n",
    "    try:\n",
    "        bbox = draw.textbbox((0,0), label_text, font=font)\n",
    "        tw, th = bbox[2]-bbox[0], bbox[3]-bbox[1]\n",
    "    except:\n",
    "        tw = int(draw.textlength(label_text, font=font)); th = 24\n",
    "\n",
    "    bw, bh = tw + 2*margin, th + 2*margin\n",
    "    draw.rectangle([(10,10),(10+bw,10+bh)], fill=(0,0,0,180))\n",
    "    draw.text((10+margin,10+margin), label_text, font=font, fill=(255,255,255))\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    img.save(out_path)\n",
    "\n",
    "def plot_counts_bar(class_counts: Dict[str,int], out_png: Path):\n",
    "    labels = list(class_counts.keys())\n",
    "    values = [class_counts[k] for k in labels]\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.bar(labels, values)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylabel(\"# Images\")\n",
    "    plt.title(\"Predicted Class Counts\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def infer_ground_truth_from_subfolders(files: List[Path], known_classes: List[str]) -> Dict[str, str]:\n",
    "\n",
    "    lowercase_classes = {c.lower(): c for c in known_classes}\n",
    "    gt = {}\n",
    "    for f in files:\n",
    "        # parent name as possible class\n",
    "        parent_name = f.parent.name.lower()\n",
    "        gt[f.as_posix()] = lowercase_classes.get(parent_name, \"\")\n",
    "    return gt\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    ensure_artifacts()\n",
    "\n",
    "    print(\"[INFO] Loading model and classes…\")\n",
    "    model = load_model(str(MODEL_H5))\n",
    "    class_indices, idx_to_class, ordered_classes = load_class_indices(CLASS_PKL)\n",
    "    num_classes = len(ordered_classes)\n",
    "    k = min(TOP_K, num_classes)\n",
    "    print(\"[INFO] Classes:\", ordered_classes)\n",
    "\n",
    "    print(f\"[INFO] Scanning Demo folder: {DEMO_DIR}\")\n",
    "    files = list_images(DEMO_DIR)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No images found under: {DEMO_DIR}\")\n",
    "    print(f\"[INFO] Found {len(files)} image(s)\")\n",
    "\n",
    "    # Optional: try to infer ground-truth from subfolder names for a quick accuracy\n",
    "    inferred_gt = infer_ground_truth_from_subfolders(files, ordered_classes)\n",
    "\n",
    "    results = []\n",
    "    class_counts = collections.Counter()\n",
    "    correct = total = 0\n",
    "\n",
    "    for i, img_path in enumerate(files, start=1):\n",
    "        arr = load_tensor(img_path)\n",
    "        probs = model.predict(arr, verbose=0)[0]  # (C,)\n",
    "\n",
    "        top_idx = np.argsort(probs)[::-1][:k]\n",
    "        top_classes = [idx_to_class[int(t)] for t in top_idx]\n",
    "        top_scores  = [float(probs[int(t)]) for t in top_idx]\n",
    "        pred_class, pred_conf = top_classes[0], top_scores[0]\n",
    "\n",
    "        class_counts[pred_class] += 1\n",
    "\n",
    "        # Quick top-1 accuracy if we could infer GT\n",
    "        gt = inferred_gt.get(img_path.as_posix(), \"\")\n",
    "        if gt:\n",
    "            total += 1\n",
    "            if gt == pred_class:\n",
    "                correct += 1\n",
    "\n",
    "        row = {\n",
    "            \"file\": str(img_path),\n",
    "            \"pred_class\": pred_class,\n",
    "            \"confidence\": round(pred_conf, 6),\n",
    "            \"topk\": [{\"class\": c, \"p\": round(s,6)} for c, s in zip(top_classes, top_scores)],\n",
    "            \"inferred_ground_truth\": gt\n",
    "        }\n",
    "        # also flatten top-k columns for CSV convenience\n",
    "        for j, (c,s) in enumerate(zip(top_classes, top_scores), start=1):\n",
    "            row[f\"top{j}_class\"] = c\n",
    "            row[f\"top{j}_p\"] = round(s, 6)\n",
    "        results.append(row)\n",
    "\n",
    "        if ANNOTATE:\n",
    "            try:\n",
    "                label = f\"{pred_class} ({pred_conf*100:.1f}%)\"\n",
    "                out_img = ANN_DIR / f\"{img_path.stem}_pred.png\"\n",
    "                annotate_image(img_path, label, out_img)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Annotate failed for {img_path.name}: {e}\")\n",
    "\n",
    "        print(f\"[{i}/{len(files)}] {img_path.name} → {pred_class} ({pred_conf*100:.2f}%)\")\n",
    "\n",
    "    # Save JSON\n",
    "    with open(JSON_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"[INFO] Saved JSON → {JSON_OUT}\")\n",
    "\n",
    "    # Save CSV\n",
    "    fieldnames = list(results[0].keys())\n",
    "    with open(CSV_OUT, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r in results:\n",
    "            writer.writerow(r)\n",
    "    print(f\"[INFO] Saved CSV → {CSV_OUT}\")\n",
    "\n",
    "    # Save class counts bar chart\n",
    "    plot_counts_bar(dict(class_counts), BAR_PNG)\n",
    "    print(f\"[INFO] Saved class count bar chart → {BAR_PNG}\")\n",
    "\n",
    "    # Save summary\n",
    "    summary = {\n",
    "        \"demo_dir\": str(DEMO_DIR),\n",
    "        \"num_images\": len(files),\n",
    "        \"class_counts\": dict(class_counts),\n",
    "        \"annotated_dir\": str(ANN_DIR) if ANNOTATE else \"\",\n",
    "    }\n",
    "    if total > 0:\n",
    "        summary[\"top1_accuracy_inferred\"] = round(correct / total, 6)\n",
    "        summary[\"num_with_inferred_gt\"] = total\n",
    "\n",
    "    with open(SUMMARY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(f\"[INFO] Saved summary → {SUMMARY_JSON}\")\n",
    "\n",
    "    if ANNOTATE:\n",
    "        print(f\"[INFO] Annotated images saved in → {ANN_DIR}\")\n",
    "\n",
    "    print(\"\\n[DONE] Prediction complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8eb9dd-0326-48e7-8fae-7023677a47b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
